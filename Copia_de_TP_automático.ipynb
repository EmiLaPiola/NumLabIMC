{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmiLaPiola/NumLabIMC/blob/main/Copia_de_TP_autom%C3%A1tico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5fMYLkWoUlM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "5ec5e96a-8481-4992-9da2-f96b6958fbc8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2603782109.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_precision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score\n",
        "\n",
        "df = pd.read_csv('data.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nueva sección"
      ],
      "metadata": {
        "id": "Ht9qOMvP6uMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EMPEZAMOS CON EL SUPER MEGA TP DE LAS MAS LINDAS"
      ],
      "metadata": {
        "id": "phdI91MGoYZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 1\n",
        "Separación de datos\n",
        "\n",
        "Evaluar y justificar cómo separarán sus datos para desarrollo y para evaluación. ¿Qué consideraciones tuvieron en cuenta para realizar esta división?\n",
        "\n",
        "Importante: en este punto no está permitido dividir la base de datos utilizando la función train_test_split de sklearn. Deben decidir e implementar la separación."
      ],
      "metadata": {
        "id": "M6AABjMxod0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero separaremos las 500 instancias en un conjunto para utilizarlos como datos de desarrollo y otros para evaluación. Primero nos fijamos cuantas isntancias son positivas , y cuantas negativas . Nos fijamso en la variable \"target\" cuales son positivas y cuales no ."
      ],
      "metadata": {
        "id": "Z7Yy2-VnraDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conteo = df['target'].value_counts()\n",
        "\n",
        "print(conteo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eARoHw6Br9Rp",
        "outputId": "fddb004e-5193-49c0-d296-063e2febe543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target\n",
            "0    353\n",
            "1    147\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El data set esta muy desbalanceado, hay mucha mas instancias con mal pronostico y solo un 29.4% de los datos tienen buen pronostico. Asi que para separar nuestros datos no lo haremos al azar. La separacion sera estratificada para que la proporicon de la clase minoritaria ( buen pronostico) sea preserve en ambos conjuntos. Utilizaremos 80 % de los datos para train y 20 % para control."
      ],
      "metadata": {
        "id": "xjXk6xUMsMxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tenemos 400 instancias en desarrolo set y 100 en el control set.  Tomamos el 20% de los datos positivos y 20% de datos negativos para ponerlos en el set de control ."
      ],
      "metadata": {
        "id": "OhK6YEBS64dg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos dos data set distintos . Uno destinado para el desarrolo y otro para la evaluación."
      ],
      "metadata": {
        "id": "q3aejzrKvTbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Separamos positivos y negativos\n",
        "positivos = df[df['target'] == True]\n",
        "negativos = df[df['target'] == False]\n",
        "\n",
        "# Seleccionamos al azar el 20% para control\n",
        "control_positivos = positivos.sample(frac=0.2, random_state=42)\n",
        "control_negativos = negativos.sample(frac=0.2, random_state=42)\n",
        "\n",
        "# Concatenamos el set de control\n",
        "NO_TOCAR_set = pd.concat([control_positivos, control_negativos])   # NO TOCAR\n",
        "\n",
        "# El resto de los datos queda para desarrollo\n",
        "desarrollo_set = df.drop(NO_TOCAR_set.index)\n",
        "\n",
        "print(f\"Desarrollo: {len(desarrollo_set)} instancias\")\n",
        "print(f\"Control: {len(NO_TOCAR_set)} instancias\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOZg1sYjs6ix",
        "outputId": "2863c301-9ee6-4b59-f25f-44a3668089e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Desarrollo: 400 instancias\n",
            "Control: 100 instancias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 2"
      ],
      "metadata": {
        "id": "RHIPrs1_wLHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) Entrenar un árbol de decisión con altura máxima 3 y el resto de los hiperparámetros en default."
      ],
      "metadata": {
        "id": "25DEI8hlxiiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estimar la performance del modelo utilizando K-fold cross validation con K=5, con las métricas Accuracy, Area Under the Precision-Recall Curve (AUPRC), y Area Under the Receiver Operating Characteristic Curve (AUCROC)."
      ],
      "metadata": {
        "id": "G7QpE9sdyXLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Ventajas de usar validación cruzada estratificada:\n",
        "\n",
        "Mantiene la proporción de clases (True/False, o cualquiera que sea tu target) en cada fold.\n",
        "\n",
        "Evita folds desbalanceados, que podrían llevar a:\n",
        "\n",
        "Modelos entrenados solo con una clase (si el dataset es desbalanceado).\n",
        "\n",
        "Métricas engañosas por una mala distribución de clases."
      ],
      "metadata": {
        "id": "YSfLXpnPzggz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacemos 5 folds estratificados para que haya las mismas proprciones de clases minoritarias y mayoritarias en todos los folds.\n",
        "\n",
        "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state = 22)\n",
        "\n",
        "# Separamos de nuestro data set la columna \"target\"\n",
        "\n",
        "desarrollo_set = desarrollo_set.reset_index(drop=True)\n",
        "x_desarrollo = desarrollo_set.drop('target', axis=1)\n",
        "y_desarrollo = desarrollo_set['target']\n",
        "\n",
        "vector_accuracy_validacion= []  # aca vamos a guardar los resultados de acurracy para cada fold\n",
        "vector_accuracy_train= []\n",
        "\n",
        "vector_auprc_validacion = []\n",
        "vector_auprc_train  = []\n",
        "\n",
        "vector_auroc_validacion=[]\n",
        "vector_auroc_train = []\n",
        "\n",
        "# vector de prediciones\n",
        "y_pred = np.empty(y_desarrollo.shape)\n",
        "y_pred.fill(np.nan)\n",
        "\n",
        "y_pred_prob = np.empty(y_desarrollo.shape)\n",
        "y_pred_prob.fill(np.nan)\n",
        "\n",
        "\n",
        "# generamos para cada fold una predicción\n",
        "for train_index, test_index in folds.split(x_desarrollo,y_desarrollo):\n",
        "\n",
        "        #saco el fold que no uso para entrenar\n",
        "        kf_X_train, kf_X_test = x_desarrollo.iloc[train_index], x_desarrollo.iloc[test_index]\n",
        "        kf_y_train, kf_y_test = y_desarrollo.iloc[train_index], y_desarrollo.iloc[test_index]\n",
        "\n",
        "        # arbol fit con los datos de train\n",
        "        arbol = DecisionTreeClassifier(max_depth=3)\n",
        "        arbol.fit(kf_X_train, kf_y_train)\n",
        "        y_pred_prob[test_index] = arbol.predict_proba(kf_X_test)[:, 1]\n",
        "\n",
        "        # Hacemos las predicciones\n",
        "        predictions = arbol.predict(kf_X_test)\n",
        "        y_pred[test_index] = predictions\n",
        "\n",
        "        # Métricas que nos piden comparar :  accuracy , AUPRC , AUCROC\n",
        "\n",
        "        # Acuracy\n",
        "\n",
        "        vector_accuracy_validacion.append(accuracy_score(kf_y_test, predictions))\n",
        "        vector_accuracy_train.append(accuracy_score(kf_y_train, arbol.predict(kf_X_train)))\n",
        "\n",
        "\n",
        "\n",
        "        # Curva AUPRC\n",
        "        auprc = average_precision_score(kf_y_test, arbol.predict_proba(kf_X_test)[:, 1])\n",
        "        vector_auprc_validacion.append(auprc)\n",
        "        vector_auprc_train.append(average_precision_score(kf_y_train, arbol.predict_proba(kf_X_train)[:, 1]))\n",
        "\n",
        "        # Curva AUROC\n",
        "        auc_roc = roc_auc_score(kf_y_test, arbol.predict_proba(kf_X_test)[:, 1])\n",
        "        vector_auroc_validacion.append(auc_roc)\n",
        "        vector_auroc_train.append(roc_auc_score(kf_y_train, arbol.predict_proba(kf_X_train)[:, 1]))\n",
        "\n",
        "\n",
        "\n",
        "# calculamos el acuracy\n",
        "print(vector_accuracy_validacion)\n",
        "print(\"El promedio de los accuracys por fold es:\", round(np.mean(vector_accuracy_validacion),3))\n",
        "print(\"El accuracy global es de : \", round(accuracy_score(y_desarrollo, y_pred),3))\n",
        "\n",
        "\n",
        "# vamos con el auprc\n",
        "\n",
        "print(vector_auprc_validacion)\n",
        "print(\"El promedio de los Area Under the Precision-Recall Curve por fold es:\", round(np.mean(vector_auprc_validacion),3))\n",
        "print(\"El Area Under the Precision-Recall Curvec global es de : \", round(average_precision_score(y_desarrollo, y_pred),3))\n",
        "\n",
        "\n",
        "# calculamos el auroc\n",
        "\n",
        "print(vector_auroc_validacion)\n",
        "print(\"El promedio de los Area Under the Receiver Operating Characteristic Curve por fold es:\", round(np.mean(vector_auroc_validacion),3))\n",
        "print(\"El Area Under the Receiver Operating Characteristic Curve global es de : \", round(roc_auc_score(y_desarrollo, y_pred),3))\n",
        "\n",
        "print(\"AUPRC global:\", round(average_precision_score(y_desarrollo, y_pred_prob), 3))\n",
        "print(\"AUROC global:\", round(roc_auc_score(y_desarrollo, y_pred_prob), 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4bmfgMMyclj",
        "outputId": "c4ad8ded-c3da-47d8-ce7c-3c75d534914d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.65, 0.75, 0.7125, 0.6875, 0.75]\n",
            "El promedio de los accuracys por fold es: 0.71\n",
            "El accuracy global es de :  0.71\n",
            "[np.float64(0.4064932301024906), np.float64(0.5160103785103786), np.float64(0.42648256671089485), np.float64(0.36316136190278064), np.float64(0.514136641278585)]\n",
            "El promedio de los Area Under the Precision-Recall Curve por fold es: 0.445\n",
            "El Area Under the Precision-Recall Curvec global es de :  0.36\n",
            "[np.float64(0.6164434523809524), np.float64(0.6863839285714286), np.float64(0.6748511904761905), np.float64(0.6372997711670481), np.float64(0.7784134248665141)]\n",
            "El promedio de los Area Under the Receiver Operating Characteristic Curve por fold es: 0.679\n",
            "El Area Under the Receiver Operating Characteristic Curve global es de :  0.59\n",
            "AUPRC global: 0.431\n",
            "AUROC global: 0.686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tabla_resultados = pd.DataFrame({\n",
        "    \"Permutación\": list(range(1, len(vector_accuracy_validacion)+1)),\n",
        "    \"Accuracy (training)\": [round(x,3) for x in vector_accuracy_train],\n",
        "    \"Accuracy (validación)\": [round(x,3) for x in vector_accuracy_validacion],\n",
        "    \"AUPRC (training)\": [round(x,3) for x in vector_auprc_train],\n",
        "    \"AUPRC (validación)\": [round(x,3) for x in vector_auprc_validacion],\n",
        "    \"AUC ROC (training)\": [round(x,3) for x in vector_auroc_train],\n",
        "    \"AUC ROC (validación)\": [round(x,3) for x in vector_auroc_validacion]\n",
        "})\n",
        "\n",
        "# Fila de promedios\n",
        "promedios = {\n",
        "    \"Permutación\": \"Promedio\",\n",
        "    \"Accuracy (training)\": round(np.mean(vector_accuracy_train), 3),\n",
        "    \"Accuracy (validación)\": round(np.mean(vector_accuracy_validacion), 3),\n",
        "    \"AUPRC (training)\": round(np.mean(vector_auprc_train), 3),\n",
        "    \"AUPRC (validación)\": round(np.mean(vector_auprc_validacion), 3),\n",
        "    \"AUC ROC (training)\": round(np.mean(vector_auroc_train), 3),\n",
        "    \"AUC ROC (validación)\": round(np.mean(vector_auroc_validacion), 3)\n",
        "}\n",
        "\n",
        "# Fila de métricas globales\n",
        "globales = {\n",
        "    \"Permutación\": \"Global\",\n",
        "    \"Accuracy (training)\": \"(NO)\",\n",
        "    \"Accuracy (validación)\": round(accuracy_score(y_desarrollo, y_pred), 3),\n",
        "    \"AUPRC (training)\": \"(NO)\",\n",
        "    \"AUPRC (validación)\": round(average_precision_score(y_desarrollo, y_pred), 3),\n",
        "    \"AUC ROC (training)\": \"(NO)\",\n",
        "    \"AUC ROC (validación)\": round(roc_auc_score(y_desarrollo, y_pred), 3)\n",
        "}\n",
        "\n",
        "tabla_resultados = pd.concat([tabla_resultados,\n",
        "                              pd.DataFrame([promedios]),\n",
        "                              pd.DataFrame([globales])],\n",
        "                             ignore_index=True)\n",
        "\n",
        "print(tabla_resultados)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEXnkxouc9ny",
        "outputId": "c621d8d6-1fe6-450e-a2dc-0e4664404cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Permutación Accuracy (training)  Accuracy (validación) AUPRC (training)  \\\n",
            "0           1               0.856                  0.650             0.72   \n",
            "1           2               0.853                  0.750            0.713   \n",
            "2           3               0.853                  0.713            0.704   \n",
            "3           4               0.847                  0.688            0.749   \n",
            "4           5               0.844                  0.750            0.745   \n",
            "5    Promedio               0.851                  0.710            0.726   \n",
            "6      Global                (NO)                  0.710             (NO)   \n",
            "\n",
            "   AUPRC (validación) AUC ROC (training)  AUC ROC (validación)  \n",
            "0               0.406              0.874                 0.616  \n",
            "1               0.516              0.842                 0.686  \n",
            "2               0.426              0.829                 0.675  \n",
            "3               0.363              0.855                 0.637  \n",
            "4               0.514              0.848                 0.778  \n",
            "5               0.445              0.849                 0.679  \n",
            "6               0.360               (NO)                 0.590  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La clase positiva es la clase minoritaria . La AUPRC (Area Under Precision-Recall Curve) es muy sensible al desbalance de clases. Y como tenemos muy pocos positivos tiene sentido que nos de un promedio global muy bajo en la validacion de aupcr  # anotar cositas de las diapos de clase sobre estos temas xd .\n",
        "Tambien tiene todo el sentido del mundo que eln los folds de entrenamiento nos de mucho mejor que en los sets de validacion ( trivial ) . Tanto roc como aupcr son sensibles a data set debalanceados como vimos en clase ."
      ],
      "metadata": {
        "id": "HFQQZkDlhIH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dDTi7d0Aj4Cp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tiene sentido que el promedio de igual al global en este caso particular porque los folds tienen todos el mismo tamaño... desarrollar :)"
      ],
      "metadata": {
        "id": "fbEj0_z6tP_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## como la clase mayoritaria son los negativos invertimos las etiquetas\n",
        "## pruebitas extras cambiando clases xd\n",
        "##  cambiando las etiquetas de las predicciones observamos que las metricas son mejroes\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ---- Datos ----\n",
        "desarrollo_set = desarrollo_set.reset_index(drop=True)\n",
        "x_desarrollo = desarrollo_set.drop('target', axis=1)\n",
        "y_desarrollo = desarrollo_set['target']\n",
        "\n",
        "# Invertimos la clase: la clase mayoritaria = 1, minoritaria = 0\n",
        "y_desarrollo_invertida = 1 - y_desarrollo\n",
        "\n",
        "# ---- Estratificación ----\n",
        "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=22)\n",
        "\n",
        "# ---- Vectores para métricas ----\n",
        "vector_accuracy_validacion = []\n",
        "vector_accuracy_train = []\n",
        "\n",
        "vector_auprc_validacion = []\n",
        "vector_auprc_train = []\n",
        "\n",
        "vector_auroc_validacion = []\n",
        "vector_auroc_train = []\n",
        "\n",
        "# ---- Predicciones globales ----\n",
        "y_pred = np.empty(y_desarrollo.shape)\n",
        "y_pred.fill(np.nan)\n",
        "\n",
        "y_pred_prob = np.empty(y_desarrollo.shape)\n",
        "y_pred_prob.fill(np.nan)\n",
        "\n",
        "# ---- Loop por fold ----\n",
        "for train_index, test_index in folds.split(x_desarrollo, y_desarrollo_invertida):\n",
        "    kf_X_train, kf_X_test = x_desarrollo.iloc[train_index], x_desarrollo.iloc[test_index]\n",
        "    kf_y_train, kf_y_test = y_desarrollo_invertida.iloc[train_index], y_desarrollo_invertida.iloc[test_index]\n",
        "\n",
        "    # Entrenar modelo\n",
        "    arbol = DecisionTreeClassifier(max_depth=3)\n",
        "    arbol.fit(kf_X_train, kf_y_train)\n",
        "\n",
        "    # Predicciones discretas y probabilidades\n",
        "    y_pred_fold = arbol.predict(kf_X_test)\n",
        "    y_pred_prob_fold = arbol.predict_proba(kf_X_test)[:,1]\n",
        "\n",
        "    # Guardar global\n",
        "    y_pred[test_index] = y_pred_fold\n",
        "    y_pred_prob[test_index] = y_pred_prob_fold\n",
        "\n",
        "    # Métricas por fold\n",
        "    vector_accuracy_validacion.append(accuracy_score(kf_y_test, y_pred_fold))\n",
        "    vector_accuracy_train.append(accuracy_score(kf_y_train, arbol.predict(kf_X_train)))\n",
        "\n",
        "    vector_auprc_validacion.append(average_precision_score(kf_y_test, y_pred_prob_fold))\n",
        "    vector_auprc_train.append(average_precision_score(kf_y_train, arbol.predict_proba(kf_X_train)[:,1]))\n",
        "\n",
        "    vector_auroc_validacion.append(roc_auc_score(kf_y_test, y_pred_prob_fold))\n",
        "    vector_auroc_train.append(roc_auc_score(kf_y_train, arbol.predict_proba(kf_X_train)[:,1]))\n",
        "\n",
        "# ---- Promedios por fold ----\n",
        "promedio_accuracy_train = np.mean(vector_accuracy_train)\n",
        "promedio_accuracy_val = np.mean(vector_accuracy_validacion)\n",
        "\n",
        "promedio_auprc_train = np.mean(vector_auprc_train)\n",
        "promedio_auprc_val = np.mean(vector_auprc_validacion)\n",
        "\n",
        "promedio_auroc_train = np.mean(vector_auroc_train)\n",
        "promedio_auroc_val = np.mean(vector_auroc_validacion)\n",
        "\n",
        "# ---- Score global ----\n",
        "accuracy_global = accuracy_score(y_desarrollo_invertida, y_pred)\n",
        "auprc_global = average_precision_score(y_desarrollo_invertida, y_pred_prob)\n",
        "auroc_global = roc_auc_score(y_desarrollo_invertida, y_pred_prob)\n",
        "\n",
        "# ---- Crear tabla ----\n",
        "tabla = pd.DataFrame({\n",
        "    'Fold': list(range(1,6)),\n",
        "    'Accuracy_train': vector_accuracy_train,\n",
        "    'Accuracy_validacion': vector_accuracy_validacion,\n",
        "    'AUPRC_train': vector_auprc_train,\n",
        "    'AUPRC_validacion': vector_auprc_validacion,\n",
        "    'AUROC_train': vector_auroc_train,\n",
        "    'AUROC_validacion': vector_auroc_validacion\n",
        "})\n",
        "\n",
        "# Agregar fila de promedios\n",
        "tabla_promedios = pd.DataFrame({\n",
        "    'Fold': ['Promedio'],\n",
        "    'Accuracy_train': [promedio_accuracy_train],\n",
        "    'Accuracy_validacion': [promedio_accuracy_val],\n",
        "    'AUPRC_train': [promedio_auprc_train],\n",
        "    'AUPRC_validacion': [promedio_auprc_val],\n",
        "    'AUROC_train': [promedio_auroc_train],\n",
        "    'AUROC_validacion': [promedio_auroc_val]\n",
        "})\n",
        "\n",
        "# Agregar fila de global\n",
        "tabla_global = pd.DataFrame({\n",
        "    'Fold': ['Global'],\n",
        "    'Accuracy_train': [np.nan],\n",
        "    'Accuracy_validacion': [accuracy_global],\n",
        "    'AUPRC_train': [np.nan],\n",
        "    'AUPRC_validacion': [auprc_global],\n",
        "    'AUROC_train': [np.nan],\n",
        "    'AUROC_validacion': [auroc_global]\n",
        "})\n",
        "\n",
        "# Concatenar todas las filas\n",
        "tabla_final = pd.concat([tabla, tabla_promedios, tabla_global], ignore_index=True)\n",
        "print(tabla_final)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4jyLM_viVkb",
        "outputId": "7ea12b48-9cf9-4c93-d4f4-7a05122d905a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Fold  Accuracy_train  Accuracy_validacion  AUPRC_train  \\\n",
            "0         1        0.856250               0.6375     0.916560   \n",
            "1         2        0.853125               0.7500     0.890516   \n",
            "2         3        0.853125               0.7250     0.880781   \n",
            "3         4        0.846875               0.6875     0.898649   \n",
            "4         5        0.843750               0.7500     0.891900   \n",
            "5  Promedio        0.850625               0.7100     0.895681   \n",
            "6    Global             NaN               0.7100          NaN   \n",
            "\n",
            "   AUPRC_validacion  AUROC_train  AUROC_validacion  \n",
            "0          0.758307     0.873917          0.587426  \n",
            "1          0.796136     0.841838          0.686384  \n",
            "2          0.816861     0.828775          0.692708  \n",
            "3          0.802280     0.855251          0.637300  \n",
            "4          0.876708     0.847673          0.778413  \n",
            "5          0.810058     0.849491          0.676446  \n",
            "6          0.806219          NaN          0.681858  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3 Probar distintos parámetros y hacer tabla"
      ],
      "metadata": {
        "id": "zS1y77haAA9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Probamos disitnos parametros para nuestro arbol de decision\n",
        "\n",
        "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state = 22)\n",
        "\n",
        "alturas = [1,2,3,5,10,None]\n",
        "criterios = ['gini','entropy']\n",
        "accuracy_validacion = {}\n",
        "accuracy_train = {}\n",
        "\n",
        "for a in alturas:\n",
        "    for c in criterios:\n",
        "        vector_accuracy_validacion= []  # aca vamos a guardar los resultados de acurracy para cada fold de la validacion\n",
        "        vector_accuracy_train= [] # y del training\n",
        "\n",
        "        # generamos para cada fold una predicción\n",
        "        for train_index, test_index in folds.split(x_desarrollo,y_desarrollo):\n",
        "\n",
        "          #saco el fold que no uso para entrenar\n",
        "          kf_X_train, kf_X_test = x_desarrollo.iloc[train_index], x_desarrollo.iloc[test_index]\n",
        "          kf_y_train, kf_y_test = y_desarrollo.iloc[train_index], y_desarrollo.iloc[test_index]\n",
        "\n",
        "          # arbol fit con los datos de train\n",
        "          arbol = DecisionTreeClassifier(max_depth=a, criterion=c)\n",
        "          arbol.fit(kf_X_train, kf_y_train)\n",
        "\n",
        "          y_pred_val = arbol.predict(kf_X_test)\n",
        "          acc_val = accuracy_score(kf_y_test, y_pred_val)\n",
        "          vector_accuracy_validacion.append(acc_val)\n",
        "\n",
        "          y_pred_train = arbol.predict(kf_X_train)\n",
        "          acc_train = accuracy_score(kf_y_train, y_pred_train)\n",
        "          vector_accuracy_train.append(acc_train)\n",
        "\n",
        "        # Guardamos el promedio de accuracy de los folds\n",
        "        accuracy_validacion[(a, c)] = np.mean(vector_accuracy_validacion)\n",
        "        accuracy_train[(a, c)] = np.mean(vector_accuracy_train)\n",
        "\n",
        "print(accuracy_validacion)\n",
        "print(accuracy_train)\n",
        "# El mejor es gini con altura 5\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Lista para guardar filas de la tabla\n",
        "tabla_resultados = []\n",
        "\n",
        "# Recorremos las claves del diccionario (pares de (altura, criterio))\n",
        "for clave in accuracy_train.keys():\n",
        "    altura, criterio = clave\n",
        "    acc_train = accuracy_train[clave]\n",
        "    acc_val = accuracy_validacion[clave]\n",
        "\n",
        "    # Agregamos una fila a la tabla\n",
        "    tabla_resultados.append({\n",
        "        'Altura': altura,\n",
        "        'Criterio': criterio,\n",
        "        'Accuracy Train': acc_train,\n",
        "        'Accuracy Validación': acc_val\n",
        "    })\n",
        "\n",
        "# Creamos el DataFrame\n",
        "df_resultados = pd.DataFrame(tabla_resultados)\n",
        "\n",
        "# Mostramos la tabla ordenada por Altura y Criterio\n",
        "df_resultados = df_resultados.sort_values(by=['Altura', 'Criterio']).reset_index(drop=True)\n",
        "print(df_resultados)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2rSnCWDAGrT",
        "outputId": "123ee158-512f-4765-dc13-e38eb1961c9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(1, 'gini'): np.float64(0.6775), (1, 'entropy'): np.float64(0.6775), (2, 'gini'): np.float64(0.7224999999999999), (2, 'entropy'): np.float64(0.735), (3, 'gini'): np.float64(0.7075), (3, 'entropy'): np.float64(0.7150000000000001), (5, 'gini'): np.float64(0.7175), (5, 'entropy'): np.float64(0.6975), (10, 'gini'): np.float64(0.6849999999999999), (10, 'entropy'): np.float64(0.6525000000000001), (None, 'gini'): np.float64(0.7), (None, 'entropy'): np.float64(0.6725000000000001)}\n",
            "{(1, 'gini'): np.float64(0.7093750000000001), (1, 'entropy'): np.float64(0.7093750000000001), (2, 'gini'): np.float64(0.784375), (2, 'entropy'): np.float64(0.779375), (3, 'gini'): np.float64(0.850625), (3, 'entropy'): np.float64(0.825), (5, 'gini'): np.float64(0.944375), (5, 'entropy'): np.float64(0.9356250000000002), (10, 'gini'): np.float64(0.9956250000000001), (10, 'entropy'): np.float64(1.0), (None, 'gini'): np.float64(1.0), (None, 'entropy'): np.float64(1.0)}\n",
            "    Altura Criterio  Accuracy Train  Accuracy Validación\n",
            "0      1.0  entropy        0.709375               0.6775\n",
            "1      1.0     gini        0.709375               0.6775\n",
            "2      2.0  entropy        0.779375               0.7350\n",
            "3      2.0     gini        0.784375               0.7225\n",
            "4      3.0  entropy        0.825000               0.7150\n",
            "5      3.0     gini        0.850625               0.7075\n",
            "6      5.0  entropy        0.935625               0.6975\n",
            "7      5.0     gini        0.944375               0.7175\n",
            "8     10.0  entropy        1.000000               0.6525\n",
            "9     10.0     gini        0.995625               0.6850\n",
            "10     NaN  entropy        1.000000               0.6725\n",
            "11     NaN     gini        1.000000               0.7000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Claramente a medida de que aumentamos el max_depth del arbol, vemos que la accuracy en los datos de train tiende a 100%, lo que tiene sentido, ya que calsifica bien todos estos datos (los memoriza). A su vez, para los datos de test. resulta peor. observamos overfitting."
      ],
      "metadata": {
        "id": "ZFuFzoZpFSJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estuvimos probando con otros valores y llegamos a la conclusion que la mejor altura es 2, ya que no solo, comparado a la performarnce en el test con otra alturas, es mejor, sino que tambien la diferencia con los datos de training es menor, osea que esta generalizando bastante bien, y no memoriza solo los datos de entrenamiento."
      ],
      "metadata": {
        "id": "08QWSDpuGclp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Ejercicio 3\n",
        "Importante: de acá en más sólamente utilizaremos el score promedio cuando hagamos K-fold cross-validation.\n",
        "\n",
        "Para el arbol de decision elegimos jugar con los siguientes hiperparametros:\n",
        "\n",
        "\n",
        "*   Max_depth (para regular el tamaño del arbol)\n",
        "*   class_weight (para tratar el desbalanceo de clases que tenemos)\n",
        "*   criterion\n",
        "*   min_samples_split (para definir cuantos hijos vamos a tener por rama como maximo)\n",
        "\n",
        "Por otro lado, para knn elegimos:\n",
        "* La cantidad de vecinos (k)\n",
        "* weights (darle mas peso a los que estan mas cercanos y menos a los mas alejados)\n",
        "* metrics (el tipo de distancia que usamos)\n",
        "\n",
        "Finalmente para SVM:\n",
        "* C (penalizacion por clasificacion equivocada)\n",
        "* kernel (hay distintos tipos)\n",
        "* class_weight (al ser desbalanceada nos conviene jugar con esto)\n"
      ],
      "metadata": {
        "id": "gaVJn8Bbj5g0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aaZZ1gcglwDL"
      }
    }
  ]
}